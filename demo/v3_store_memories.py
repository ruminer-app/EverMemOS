"""V3 API è®°å¿†å­˜å‚¨æµ‹è¯•

åŠŸèƒ½ï¼š
1. æ¸…ç©ºæ‰€æœ‰æ•°æ®åº“æ•°æ®ï¼ˆMongoDB, Milvus, ESï¼‰
2. ä» assistant_chat_zh.json è¯»å–å¯¹è¯æ•°æ®
3. è°ƒç”¨ V3 API æå–å¹¶å­˜å‚¨è®°å¿†
"""
import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from core.di import get_bean_by_type
from infra_layer.adapters.out.persistence.document.memory.memcell import MemCell
from infra_layer.adapters.out.persistence.document.memory.episodic_memory import EpisodicMemory
from infra_layer.adapters.out.persistence.document.memory.personal_semantic_memory import PersonalSemanticMemory
from infra_layer.adapters.out.persistence.document.memory.personal_event_log import PersonalEventLog
from infra_layer.adapters.out.persistence.document.memory.semantic_memory import SemanticMemory
from infra_layer.adapters.out.search.repository.episodic_memory_milvus_repository import EpisodicMemoryMilvusRepository
from infra_layer.adapters.out.search.repository.episodic_memory_es_repository import EpisodicMemoryEsRepository


async def clear_all_data():
    """æ¸…ç©ºæ‰€æœ‰æ•°æ®åº“æ•°æ®"""
    print("=" * 100)
    print("ğŸ—‘ï¸  æ¸…ç©ºæ‰€æœ‰æ•°æ®åº“æ•°æ®")
    print("=" * 100)
    
    # 1. æ¸…ç©º MongoDB
    print("\nã€1ã€‘æ¸…ç©º MongoDB é›†åˆ...")
    
    memcell_count = await MemCell.find_all().count()
    await MemCell.find_all().delete()
    print(f"  âœ… MemCell: åˆ é™¤ {memcell_count} æ¡")
    
    episodic_count = await EpisodicMemory.find_all().count()
    await EpisodicMemory.find_all().delete()
    print(f"  âœ… EpisodicMemory: åˆ é™¤ {episodic_count} æ¡")
    
    semantic_count = await SemanticMemory.find_all().count()
    await SemanticMemory.find_all().delete()
    print(f"  âœ… SemanticMemory: åˆ é™¤ {semantic_count} æ¡")
    
    personal_semantic_count = await PersonalSemanticMemory.find_all().count()
    await PersonalSemanticMemory.find_all().delete()
    print(f"  âœ… PersonalSemanticMemory: åˆ é™¤ {personal_semantic_count} æ¡")
    
    personal_eventlog_count = await PersonalEventLog.find_all().count()
    await PersonalEventLog.find_all().delete()
    print(f"  âœ… PersonalEventLog: åˆ é™¤ {personal_eventlog_count} æ¡")
    
    # 2. æ¸…ç©º Milvus
    print("\nã€2ã€‘æ¸…ç©º Milvus æ•°æ®...")
    milvus_repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    try:
        # ç›´æ¥åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ˆä½¿ç”¨ id != "" åŒ¹é…æ‰€æœ‰è®°å½•ï¼‰
        milvus_collection = milvus_repo.collection
        
        # å…ˆæŸ¥è¯¢æ€»æ•°
        total_before = len(await milvus_collection.query(expr='id != ""', output_fields=["id"], limit=16384))
        
        # åˆ é™¤æ‰€æœ‰æ•°æ®
        await milvus_collection.delete(expr='id != ""')
        await milvus_repo.flush()
        print(f"  âœ… Milvus: åˆ é™¤ {total_before} æ¡")
    except Exception as e:
        print(f"  âš ï¸  Milvus æ¸…ç©ºå¤±è´¥: {e}")
    
    # 3. æ¸…ç©º Elasticsearch
    print("\nã€3ã€‘æ¸…ç©º Elasticsearch æ•°æ®...")
    es_repo = get_bean_by_type(EpisodicMemoryEsRepository)
    try:
        # åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ˆä½¿ç”¨ match_all æŸ¥è¯¢ï¼‰
        client = await es_repo.get_client()
        index_name = es_repo.get_index_name()
        
        response = await client.delete_by_query(
            index=index_name,
            body={"query": {"match_all": {}}},
            refresh=True
        )
        
        deleted_count = response.get('deleted', 0)
        print(f"  âœ… Elasticsearch: åˆ é™¤ {deleted_count} æ¡")
    except Exception as e:
        print(f"  âš ï¸  ES æ¸…ç©ºå¤±è´¥: {e}")
    
    print("\nâœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç©ºï¼\n")


async def load_conversation_data(file_path: str):
    """åŠ è½½å¯¹è¯æ•°æ®"""
    print("=" * 100)
    print("ğŸ“– åŠ è½½å¯¹è¯æ•°æ®")
    print("=" * 100)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå– conversation_list åˆ—è¡¨
    messages = data.get('conversation_list', [])
    conversation_meta = data.get('conversation_meta', {})
    
    print(f"\næ–‡ä»¶: {file_path}")
    print(f"å¯¹è¯è½®æ•°: {len(messages)} è½®")
    print(f"ç¾¤ç»„ID: {conversation_meta.get('group_id', 'unknown')}")
    
    # æ˜¾ç¤ºå‰3è½®å¯¹è¯
    print("\nå‰3è½®å¯¹è¯é¢„è§ˆ:")
    for i, msg in enumerate(messages[:3]):
        sender = msg.get('sender', 'unknown')
        content = msg.get('content', '')[:100]
        print(f"  [{i}] {sender}: {content}...")
    
    return data


async def extract_memories(conversation_data: dict):
    """ä½¿ç”¨ MemoryManager æå–è®°å¿†ï¼ˆä¸ test_v3_api_simple.py ç›¸åŒæ–¹å¼ï¼‰"""
    print("\n" + "=" * 100)
    print("ğŸ§  æå–è®°å¿†ï¼ˆä½¿ç”¨ MemoryManager.memorizeï¼‰")
    print("=" * 100)
    
    # ä»æ•°æ®ä¸­æå– conversation_list å’Œ meta ä¿¡æ¯
    raw_messages = conversation_data.get('conversation_list', [])
    conversation_meta = conversation_data.get('conversation_meta', {})
    group_id = conversation_meta.get('group_id', 'test_group_001')
    
    print(f"\nå¯¹è¯æ¶ˆæ¯æ•°: {len(raw_messages)} æ¡")
    print(f"ç¾¤ç»„ID: {group_id}")
    
    # åˆå§‹åŒ– MemoryManager
    from agentic_layer.memory_manager import MemoryManager
    from memory_layer.types import RawDataType
    from memory_layer.memcell_extractor.base_memcell_extractor import RawData
    from memory_layer.memory_manager import MemorizeRequest
    from common_utils.datetime_utils import from_iso_format
    
    manager = MemoryManager()
    
    # é€æ¡å¤„ç†æ¶ˆæ¯
    history = []
    saved_count = 0
    start_time = datetime.now()
    
    print("\nå¼€å§‹æå–è®°å¿†...")
    
    for idx, msg in enumerate(raw_messages):
        # æ„é€ æ¶ˆæ¯æ•°æ®
        timestamp_str = msg.get('create_time')
        if not timestamp_str:
            continue
        
        timestamp_dt = from_iso_format(timestamp_str)
        speaker_id = msg.get('sender')
        speaker_name = msg.get('sender_name', speaker_id)
        content = msg.get('content', '')
        message_id = msg.get('message_id', f"msg_{idx}")
        
        message_payload = {
            "speaker_id": speaker_id,
            "speaker_name": speaker_name,
            "content": content,
            "timestamp": timestamp_dt,
        }
        
        raw_item = RawData(
            content=message_payload,
            data_id=str(message_id),
            data_type=RawDataType.CONVERSATION,
        )
        
        # åˆå§‹åŒ–å†å²
        if not history:
            history.append(raw_item)
            continue
        
        # æ„å»ºè¯·æ±‚ - æå–æ‰€æœ‰å‚ä¸è€…
        participants = set()
        for h in history:
            speaker_id = h.content.get("speaker_id")
            if speaker_id:
                participants.add(speaker_id)
        # æ·»åŠ å½“å‰æ¶ˆæ¯çš„ speaker_id
        if message_payload.get("speaker_id"):
            participants.add(message_payload["speaker_id"])
        
        user_id_list = list(participants) if participants else ["default"]
        
        request = MemorizeRequest(
            history_raw_data_list=list(history),
            new_raw_data_list=[raw_item],
            raw_data_type=RawDataType.CONVERSATION,
            user_id_list=user_id_list,
            group_id=group_id,
            enable_semantic_extraction=True,   # å¯ç”¨è¯­ä¹‰è®°å¿†
            enable_event_log_extraction=True,  # å¯ç”¨ Event Log
        )
        
        # è°ƒç”¨ memorize
        result = await manager.memorize(request)
        
        if result:
            saved_count += 1
            print(f"  [{saved_count}] âœ… æå–æˆåŠŸï¼Œè¿”å› {len(result)} ä¸ª Memory")
            
            # é‡ç½®å†å²
            history = [raw_item]
        else:
            # ç»§ç»­ç´¯ç§¯
            history.append(raw_item)
            if len(history) > 20:
                history = history[-20:]
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\n{'='*100}")
    print("ğŸ“Š æå–ç»Ÿè®¡")
    print(f"{'='*100}")
    print(f"  æ€»æ¶ˆæ¯æ•°: {len(raw_messages)} æ¡")
    print(f"  æå– MemCell æ•°: {saved_count} ä¸ª")
    print(f"  æ€»è€—æ—¶: {duration:.2f} ç§’")
    
    print(f"\nâœ… è®°å¿†æå–å®Œæˆï¼")
    return saved_count


async def verify_storage():
    """éªŒè¯æ•°æ®å­˜å‚¨"""
    print("\n" + "=" * 100)
    print("âœ“ éªŒè¯æ•°æ®å­˜å‚¨")
    print("=" * 100)
    
    # MongoDB ç»Ÿè®¡
    print("\nã€MongoDBã€‘")
    memcell_count = await MemCell.find_all().count()
    episodic_count = await EpisodicMemory.find_all().count()
    semantic_count = await PersonalSemanticMemory.find_all().count()
    eventlog_count = await PersonalEventLog.find_all().count()
    
    print(f"  MemCell: {memcell_count} æ¡")
    print(f"  EpisodicMemory (ä¸ªäººæƒ…æ™¯): {episodic_count} æ¡")
    print(f"  PersonalSemanticMemory: {semantic_count} æ¡")
    print(f"  PersonalEventLog: {eventlog_count} æ¡")
    
    # Milvus ç»Ÿè®¡
    print("\nã€Milvusã€‘")
    milvus_repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    from agentic_layer.vectorize_service import get_vectorize_service
    
    vectorize_service = get_vectorize_service()
    query_vec = await vectorize_service.get_embedding("æµ‹è¯•")
    
    milvus_results = await milvus_repo.vector_search(
        query_vector=query_vec,
        limit=500,
    )
    
    type_stats = {}
    for result in milvus_results:
        memory_type = result.get('memory_sub_type', 'unknown')
        type_stats[memory_type] = type_stats.get(memory_type, 0) + 1
    
    print(f"  æ€»è®°å½•æ•°: {len(milvus_results)} æ¡")
    for memory_type, count in sorted(type_stats.items()):
        print(f"    - {memory_type}: {count} æ¡")
    
    # Elasticsearch ç»Ÿè®¡
    print("\nã€Elasticsearchã€‘")
    es_repo = get_bean_by_type(EpisodicMemoryEsRepository)
    import jieba
    
    es_hits = await es_repo.multi_search(
        query=list(jieba.cut("æµ‹è¯•")),
        size=500,
    )
    
    es_type_stats = {}
    for hit in es_hits:
        source = hit.get('_source', {})
        es_type = source.get('type', 'unknown')
        es_type_stats[es_type] = es_type_stats.get(es_type, 0) + 1
    
    print(f"  æ€»è®°å½•æ•°: {len(es_hits)} æ¡")
    for es_type, count in sorted(es_type_stats.items()):
        print(f"    - {es_type}: {count} æ¡")
    
    # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
    all_success = (
        memcell_count > 0 and
        len(milvus_results) > 0 and
        len(es_hits) > 0
    )
    
    if all_success:
        print("\nğŸ‰ æ•°æ®å­˜å‚¨éªŒè¯é€šè¿‡ï¼")
    else:
        print("\nâš ï¸  æ•°æ®å­˜å‚¨å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ã€‚")
    
    return all_success


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸš€" * 50)
    print("V3 API è®°å¿†å­˜å‚¨æµ‹è¯•")
    print("ğŸš€" * 50)
    
    # 1. æ¸…ç©ºæ•°æ®
    await clear_all_data()
    
    # 2. åŠ è½½å¯¹è¯æ•°æ®
    data_file = "/Users/admin/Documents/Projects/opensource/memsys-opensource/data/assistant_chat_zh.json"
    conversation_data = await load_conversation_data(data_file)
    
    # 3. æå–è®°å¿†
    success = await extract_memories(conversation_data)
    
    if not success:
        print("\nâŒ è®°å¿†æå–å¤±è´¥ï¼Œé€€å‡ºã€‚")
        return
    
    # 4. ç­‰å¾…åŒæ­¥å®Œæˆ
    print("\nâ³ ç­‰å¾…æ•°æ®åŒæ­¥å®Œæˆï¼ˆ3ç§’ï¼‰...")
    await asyncio.sleep(3)
    
    # 5. éªŒè¯å­˜å‚¨
    await verify_storage()
    
    print("\n" + "=" * 100)
    print("âœ… è®°å¿†å­˜å‚¨æµ‹è¯•å®Œæˆï¼")
    print("=" * 100)
    print("\nğŸ’¡ æç¤º: ç°åœ¨å¯ä»¥è¿è¡Œ v3_retrieve_memories.py è¿›è¡Œæ£€ç´¢æµ‹è¯•")


if __name__ == "__main__":
    asyncio.run(main())

