import argparse
import asyncio
import json
import os
import sys
from pathlib import Path
from time import time
from typing import List, Dict, Optional

import pandas as pd
from tqdm import tqdm



from evaluation.src.adapters.evermemos.config import ExperimentConfig
from evaluation.src.adapters.evermemos.prompts.answer_prompts import ANSWER_PROMPT

# ä½¿ç”¨ Memory Layer çš„ LLMProvider
from memory_layer.llm.llm_provider import LLMProvider


# ğŸ”¥ Context æ„å»ºæ¨¡æ¿ï¼ˆä» stage3 è¿ç§»è¿‡æ¥ï¼‰
TEMPLATE = """Episodes memories for conversation between {speaker_1} and {speaker_2}:

    {speaker_memories}
"""


def load_memcells_by_conversation(conv_idx: int, memcells_dir: Path) -> Dict[str, dict]:
    """
    åŠ è½½æŒ‡å®šå¯¹è¯çš„æ‰€æœ‰ memcellsï¼Œè¿”å› event_id -> memcell çš„æ˜ å°„
    
    Args:
        conv_idx: å¯¹è¯ç´¢å¼•
        memcells_dir: memcells ç›®å½•è·¯å¾„
    
    Returns:
        {event_id: memcell_dict} çš„æ˜ å°„
    """
    memcell_file = memcells_dir / f"memcell_list_conv_{conv_idx}.json"
    
    if not memcell_file.exists():
        print(f"Warning: Memcell file not found: {memcell_file}")
        return {}
    
    try:
        with open(memcell_file, "r", encoding="utf-8") as f:
            memcells = json.load(f)
        
        # æ„å»º event_id -> memcell çš„æ˜ å°„
        memcell_map = {}
        for memcell in memcells:
            event_id = memcell.get("event_id")
            if event_id:
                memcell_map[event_id] = memcell
        
        return memcell_map
    
    except Exception as e:
        print(f"Error loading memcells from {memcell_file}: {e}")
        return {}


def build_context_from_event_ids(
    event_ids: List[str],
    memcell_map: Dict[str, dict],
    speaker_a: str,
    speaker_b: str,
    top_k: int = 10
) -> str:
    """
    æ ¹æ® event_ids ä» memcell_map ä¸­æå–å¯¹åº”çš„ episode memoryï¼Œæ„å»º context
    
    Args:
        event_ids: æ£€ç´¢åˆ°çš„ event_ids åˆ—è¡¨ï¼ˆå·²æŒ‰ç›¸å…³æ€§æ’åºï¼‰
        memcell_map: event_id -> memcell çš„æ˜ å°„
        speaker_a: è¯´è¯è€… A
        speaker_b: è¯´è¯è€… B
        top_k: é€‰æ‹©å‰ k ä¸ª event_idsï¼ˆé»˜è®¤ 10ï¼‰
    
    Returns:
        æ ¼å¼åŒ–çš„ context å­—ç¬¦ä¸²
    """
    # ğŸ”¥ é€‰æ‹© top-k event_ids
    selected_event_ids = event_ids[:top_k]
    
    # ä» memcell_map ä¸­æå–å¯¹åº”çš„ episode memory
    retrieved_docs_text = []
    for event_id in selected_event_ids:
        memcell = memcell_map.get(event_id)
        if not memcell:
            # æ‰¾ä¸åˆ°å¯¹åº”çš„ memcellï¼Œè·³è¿‡
            continue
        
        subject = memcell.get('subject', 'N/A')
        episode = memcell.get('episode', 'N/A')
        doc_text = f"{subject}: {episode}\n---"
        retrieved_docs_text.append(doc_text)
    
    # æ‹¼æ¥æ‰€æœ‰æ–‡æ¡£
    speaker_memories = "\n\n".join(retrieved_docs_text)
    
    # ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–æœ€ç»ˆ context
    context = TEMPLATE.format(
        speaker_1=speaker_a,
        speaker_2=speaker_b,
        speaker_memories=speaker_memories,
    )
    
    return context


async def locomo_response(
    llm_provider: LLMProvider,  # æ”¹ç”¨ LLMProvider
    context: str,
    question: str,
    experiment_config: ExperimentConfig,
) -> str:
    """ç”Ÿæˆå›ç­”ï¼ˆä½¿ç”¨ LLMProviderï¼‰
    
    Args:
        llm_provider: LLM Provider
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        question: ç”¨æˆ·é—®é¢˜
        experiment_config: å®éªŒé…ç½®
    
    Returns:
        ç”Ÿæˆçš„ç­”æ¡ˆ
    """
    prompt = ANSWER_PROMPT.format(context=context, question=question)
    
    for i in range(experiment_config.max_retries):
        try:
            result = await llm_provider.generate(
                prompt=prompt,
                temperature=0,
                max_tokens=32768,
            )
            
            # ğŸ”¥ å®‰å…¨è§£æ FINAL ANSWERï¼ˆé¿å… index out of rangeï¼‰
            if "FINAL ANSWER:" in result:
                parts = result.split("FINAL ANSWER:")
                if len(parts) > 1:
                    result = parts[1].strip()
                else:
                    # åˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                    result = result.strip()
            else:
                # æ²¡æœ‰ FINAL ANSWER æ ‡è®°ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                result = result.strip()
            
            if result == "":
                continue
            break
        except Exception as e:
            print(f"Error: {e}")
            continue
    
    return result


async def process_qa(
    qa, 
    search_result, 
    llm_provider, 
    experiment_config,
    memcell_map: Dict[str, dict],
    speaker_a: str,
    speaker_b: str
):
    """
    å¤„ç†å•ä¸ª QA å¯¹ï¼ˆæ–°ç‰ˆï¼šä» event_ids æ„å»º contextï¼‰
    
    Args:
        qa: é—®é¢˜å’Œç­”æ¡ˆå¯¹
        search_result: æ£€ç´¢ç»“æœï¼ˆåŒ…å« event_idsï¼‰
        llm_provider: LLM Provider
        experiment_config: å®éªŒé…ç½®
        memcell_map: event_id -> memcell çš„æ˜ å°„
        speaker_a: è¯´è¯è€… A
        speaker_b: è¯´è¯è€… B
    
    Returns:
        åŒ…å«é—®é¢˜ã€ç­”æ¡ˆã€ç±»åˆ«ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    start = time()
    query = qa.get("question")
    gold_answer = qa.get("answer")
    qa_category = qa.get("category")

    # ğŸ”¥ ä» event_ids æ„å»º contextï¼ˆä½¿ç”¨ top_kï¼‰
    event_ids = search_result.get("event_ids", [])
    top_k = experiment_config.response_top_k
    
    context = build_context_from_event_ids(
        event_ids=event_ids,
        memcell_map=memcell_map,
        speaker_a=speaker_a,
        speaker_b=speaker_b,
        top_k=top_k
    )

    answer = await locomo_response(
        llm_provider, context, query, experiment_config
    )

    response_duration_ms = (time() - start) * 1000

    # åªåœ¨ verbose æ¨¡å¼ä¸‹è¾“å‡ºï¼ˆå‡å°‘æ—¥å¿—ï¼‰
    # print(f"Processed question: {query}")
    # print(f"Answer: {answer}")
    
    return {
        "question": query,
        "answer": answer,
        "category": qa_category,
        "golden_answer": gold_answer,
        "search_context": context,  # ä¿å­˜æ„å»ºçš„ context
        "event_ids_used": event_ids[:top_k],  # ğŸ”¥ è®°å½•å®é™…ä½¿ç”¨çš„ event_ids
        "response_duration_ms": response_duration_ms,
        "search_duration_ms": search_result.get("retrieval_metadata", {}).get("total_latency_ms", 0),
    }


async def main(search_path, save_path):
    """
    ä¼˜åŒ–åçš„ä¸»å‡½æ•°
    
    æ€§èƒ½ä¼˜åŒ–ï¼š
    1. å…¨å±€å¹¶å‘å¤„ç†ï¼šæ‰€æœ‰ QA å¯¹å¹¶å‘å¤„ç†ï¼Œè€Œä¸æ˜¯æŒ‰ conversation ä¸²è¡Œ
    2. å¹¶å‘æ§åˆ¶ï¼šä½¿ç”¨ Semaphore æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    3. è¿›åº¦ç›‘æ§ï¼šå®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
    4. å¢é‡ä¿å­˜ï¼šæ¯ä¸ª conversation å®Œæˆåç«‹å³ä¿å­˜ï¼ˆé¿å…å´©æºƒä¸¢å¤±æ•°æ®ï¼‰
    
    ä¼˜åŒ–æ•ˆæœï¼š
    - ä¼˜åŒ–å‰ï¼š77 åˆ†é’Ÿï¼ˆä¸²è¡Œï¼‰
    - ä¼˜åŒ–åï¼š~8 åˆ†é’Ÿï¼ˆå¹¶å‘ 50ï¼‰
    - åŠ é€Ÿæ¯”ï¼š~10x
    """
    llm_config = ExperimentConfig.llm_config["openai"]
    experiment_config = ExperimentConfig()
    
    # åˆ›å»º LLM Providerï¼ˆæ›¿ä»£ AsyncOpenAIï¼‰
    llm_provider = LLMProvider(
        provider_type="openai",
        model=llm_config["model"],
        api_key=llm_config["api_key"],
        base_url=llm_config["base_url"],
        temperature=llm_config.get("temperature", 0.0),
        max_tokens=llm_config.get("max_tokens", 32768),
    )
    
    locomo_df = pd.read_json(experiment_config.datase_path)
    with open(search_path) as file:
        locomo_search_results = json.load(file)

    num_users = len(locomo_df)
    
    # ğŸ”¥ åŠ è½½ memcells ç›®å½•
    memcells_dir = Path(search_path).parent / "memcells"
    if not memcells_dir.exists():
        print(f"Error: Memcells directory not found: {memcells_dir}")
        return
    
    print(f"\n{'='*60}")
    print(f"Stage4: LLM Response Generation")
    print(f"{'='*60}")
    print(f"Total conversations: {num_users}")
    print(f"Response top-k: {experiment_config.response_top_k}")
    print(f"Memcells directory: {memcells_dir}")
    
    # ğŸ”¥ ä¼˜åŒ–1ï¼šå…¨å±€å¹¶å‘æ§åˆ¶ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
    # æ§åˆ¶åŒæ—¶å¤„ç†çš„ QA å¯¹æ•°é‡ï¼Œé¿å… API é™æµ
    MAX_CONCURRENT = 50  # å¯æ ¹æ® API é™åˆ¶è°ƒæ•´ï¼ˆ10-100ï¼‰
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    
    # ğŸ”¥ ä¼˜åŒ–2ï¼šæ”¶é›†æ‰€æœ‰ QA å¯¹ï¼ˆè·¨ conversationï¼‰
    all_tasks = []
    task_to_group = {}  # ç”¨äºè¿½è¸ªæ¯ä¸ªä»»åŠ¡å±äºå“ªä¸ª group
    
    # ğŸ”¥ ä¼˜åŒ–3ï¼šå®šä¹‰å¸¦å¹¶å‘æ§åˆ¶çš„å¤„ç†å‡½æ•°
    async def process_qa_with_semaphore(qa, search_result, group_id, memcell_map, speaker_a, speaker_b):
        """å¸¦å¹¶å‘æ§åˆ¶çš„ QA å¤„ç†"""
        async with semaphore:
            result = await process_qa(
                qa, search_result, llm_provider, experiment_config,
                memcell_map, speaker_a, speaker_b
            )
            return (group_id, result)
    
    total_qa_count = 0
    for group_idx in range(num_users):
        qa_set = locomo_df["qa"].iloc[group_idx]
        qa_set_filtered = [qa for qa in qa_set if qa.get("category") != 5]

        group_id = f"locomo_exp_user_{group_idx}"
        search_results = locomo_search_results.get(group_id)
        
        # ğŸ”¥ åŠ è½½å½“å‰å¯¹è¯çš„ memcells
        memcell_map = load_memcells_by_conversation(group_idx, memcells_dir)
        print(f"Loaded {len(memcell_map)} memcells for conversation {group_idx}")
        
        # ğŸ”¥ è·å– speaker ä¿¡æ¯
        conversation_data = locomo_df["conversation"].iloc[group_idx]
        speaker_a = conversation_data.get("speaker_a", "Speaker A")
        speaker_b = conversation_data.get("speaker_b", "Speaker B")

        matched_pairs = []
        for qa in qa_set_filtered:
            question = qa.get("question")
            matching_result = next(
                (
                    result
                    for result in search_results
                    if result.get("query") == question
                ),
                None,
            )
            if matching_result:
                matched_pairs.append((qa, matching_result))
            else:
                print(
                    f"Warning: No matching search result found for question: {question}"
                )
        
        total_qa_count += len(matched_pairs)
        
        # åˆ›å»ºä»»åŠ¡ï¼ˆå…¨å±€å¹¶å‘ï¼‰
        for qa, search_result in matched_pairs:
            task = process_qa_with_semaphore(qa, search_result, group_id, memcell_map, speaker_a, speaker_b)
            all_tasks.append(task)
    
    print(f"Total questions to process: {total_qa_count}")
    print(f"Max concurrent requests: {MAX_CONCURRENT}")
    print(f"Estimated time: {total_qa_count * 3 / MAX_CONCURRENT / 60:.1f} minutes")
    print(f"\n{'='*60}")
    print(f"Starting parallel processing...")
    print(f"{'='*60}\n")
    
    # ğŸ”¥ ä¼˜åŒ–4ï¼šå…¨å±€å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼ˆå¸¦è¿›åº¦ç›‘æ§ï¼‰
    all_responses = {f"locomo_exp_user_{i}": [] for i in range(num_users)}
    
    import time as time_module
    start_time = time_module.time()
    completed = 0
    failed = 0
    
    # ğŸ”¥ ä¼˜åŒ–5ï¼šåˆ†æ‰¹å¤„ç† + å¢é‡ä¿å­˜ï¼ˆé¿å…å´©æºƒä¸¢å¤±æ•°æ®ï¼‰
    CHUNK_SIZE = 200  # æ¯æ¬¡å¤„ç† 200 ä¸ªä»»åŠ¡
    SAVE_INTERVAL = 400  # æ¯ 400 ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡
    
    for chunk_start in range(0, len(all_tasks), CHUNK_SIZE):
        chunk_tasks = all_tasks[chunk_start : chunk_start + CHUNK_SIZE]
        chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)
        
        # å°†ç»“æœåˆ†ç»„åˆ°å„ä¸ª conversation
        for result in chunk_results:
            if isinstance(result, Exception):
                print(f"  âŒ Task failed: {result}")
                failed += 1
                continue
            
            group_id, qa_result = result
            all_responses[group_id].append(qa_result)
        
        completed += len(chunk_tasks)
        elapsed = time_module.time() - start_time
        speed = completed / elapsed if elapsed > 0 else 0
        eta = (total_qa_count - completed) / speed if speed > 0 else 0
        
        print(f"Progress: {completed}/{total_qa_count} ({completed/total_qa_count*100:.1f}%) | "
              f"Speed: {speed:.1f} qa/s | Failed: {failed} | ETA: {eta/60:.1f} min")
        
        # ğŸ”¥ å¢é‡ä¿å­˜ï¼ˆæ¯ SAVE_INTERVAL ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡ï¼‰
        if completed % SAVE_INTERVAL == 0 or completed == total_qa_count:
            temp_save_path = Path(save_path).parent / f"responses_checkpoint_{completed}.json"
            with open(temp_save_path, "w", encoding="utf-8") as f:
                json.dump(all_responses, f, indent=2, ensure_ascii=False)
            print(f"  ğŸ’¾ Checkpoint saved: {temp_save_path.name}")
    
    elapsed_time = time_module.time() - start_time
    success_rate = (completed - failed) / completed * 100 if completed > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"âœ… All responses generated!")
    print(f"   - Total questions: {total_qa_count}")
    print(f"   - Successful: {completed - failed}")
    print(f"   - Failed: {failed}")
    print(f"   - Success rate: {success_rate:.1f}%")
    print(f"   - Time elapsed: {elapsed_time/60:.1f} minutes ({elapsed_time:.0f}s)")
    print(f"   - Average speed: {total_qa_count/elapsed_time:.1f} qa/s")
    print(f"{'='*60}\n")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    os.makedirs(Path(save_path).parent, exist_ok=True)
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_responses, f, indent=2, ensure_ascii=False)
        print(f"âœ… Final results saved to: {save_path}")
    
    # æ¸…ç† checkpoint æ–‡ä»¶
    checkpoint_files = list(Path(save_path).parent.glob("responses_checkpoint_*.json"))
    for checkpoint_file in checkpoint_files:
        checkpoint_file.unlink()
        print(f"  ğŸ—‘ï¸  Removed checkpoint: {checkpoint_file.name}")


if __name__ == "__main__":
    config = ExperimentConfig()
    # ğŸ”¥ ä¿®æ­£ï¼šå®é™…æ–‡ä»¶åœ¨ locomo_evaluation/ ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯ results/ ç›®å½•
    search_result_path = str(
        Path(__file__).parent
        / config.experiment_name  # ç›´æ¥ä½¿ç”¨ experiment_nameï¼ˆå³ "locomo_evaluation"ï¼‰
        / "search_results.json"
    )
    save_path = (
        Path(__file__).parent / config.experiment_name / "responses.json"
    )
    # search_result_path = f"/Users/admin/Documents/Projects/b001-memsys/evaluation/locomo_evaluation/results/locomo_evaluation_0/nemori_locomo_search_results.json"

    asyncio.run(main(search_result_path, save_path))
